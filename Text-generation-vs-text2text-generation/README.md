# Text Generation Models

![gpt2](https://github.com/SharathHebbar/Transformers/blob/main/Text-generation-vs-text2text-generation/Text-generation/assets/gpt2.png)
- Text Generation, also known as Causal Language Modeling, is the process of generating text that closely resembles human writing. 
- It utilizes a Decoder-only architecture and operates in a left-to-right context.
- Text Generation is often employed for tasks such as sentence completion and generating the next lines of poetry when given a few lines as input. 
- Examples of Text Generation models include the GPT family, BLOOM, and PaLM, which find applications in Chatbots, Text Completion, and content generation.

- For Code refer [Text Generation](https://github.com/SharathHebbar/Transformers/blob/main/Text-generation-vs-text2text-generation/Text-generation/text-generation.ipynb)

# Text2Text Generation Models

![T5](https://github.com/SharathHebbar/Transformers/blob/main/Text-generation-vs-text2text-generation/Text2Text-generation/assets/t5.png)
- Text-to-Text Generation, also known as Sequence-to-Sequence Modeling, is the process of converting one piece of text into another.
- It relies on an encoder-decoder architecture and operates in both right-to-left and left-to-right contexts.
- Text-to-text generation is frequently employed for tasks such as translating English sentences into French or summarizing lengthy paragraphs.
- Examples of Text Generation models include T5 and BART, which are commonly used in question-answering, Translation, and Summarization tasks.

- For Code refer [Text2Text Generation](https://github.com/SharathHebbar/Transformers/blob/main/Text-generation-vs-text2text-generation/Text2Text-generation/text2text-generation.ipynb)