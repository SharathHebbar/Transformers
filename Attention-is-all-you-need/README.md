# Attention is all you need

1. [Attention Mechanism](https://github.com/SharathHebbar/Transformers/tree/main/Attention-is-all-you-need/Attention%20Mechanism)
2. [Positional Encoding]()
3. [Transformers from scratch](https://github.com/SharathHebbar/Transformers/tree/main/Attention-is-all-you-need/Transformers%20from%20scratch)