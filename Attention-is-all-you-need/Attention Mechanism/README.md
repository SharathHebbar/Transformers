# Attention Mechanism

1. [Self Attention](https://github.com/SharathHebbar/Transformers/blob/main/Attention-is-all-you-need/Attention%20Mechanism/self-attention.ipynb)
2. [Masked Self Attention](https://github.com/SharathHebbar/Transformers/blob/main/Attention-is-all-you-need/Attention%20Mechanism/masked-self-attention.ipynb)
3. [Multi-Head Attention](https://github.com/SharathHebbar/Transformers/blob/main/Attention-is-all-you-need/Attention%20Mechanism/multi-headed-attention.ipynb)