# Background

<table>

<tr>
<th>Aspects</th>
<th>LSTM</th>
<th>Transformers</th>
</tr>

<tr>
<td>Processing Methods</td>
<td>Sequential (processess one token at a time)</td>
<td>Parallel (processes all tokens simultaneously)</td>
</tr>

<tr>
<td>Handling Dependencies</td>
<td>Can struggle with long-term dependencies</td>
<td>Excels at capturing long-term dependencies</td>
</tr>

<tr>
<td>Attention Mechanism</td>
<td>No inherent attention, can be added separately</td>
<td>Inherent multi-headed self-attention mechanism</td>
</tr>

<tr>
<td>Efficiency and Scalability</td>
<td>Less efficient and scalable for very long sequences</td>
<td>More efficient and scalable for long sequences and large datasets</td>
</tr>

</table>